You are an expert software architect working on a Multi-source verified, AI-scored lists of B2B SaaS companies that raised funding 60-90 days ago. This project will involve multiple ai agents.

Please write task 6 for FSQâ€‘008 (Feedback Resolver), Integration Notes
	â€¢	Optional: add a TODO/README note describing where FSQâ€‘008 slots into the day1 pipeline (after normalize_and_resolve, before unified_verify). Actual wiring can be a follow-on task if needed

and using this task template:

### FINAL OPTIMIZED TASK TEMPLATE (Codex-Ready)

Task [ID]: [Title]
	â€¢	Status: [Ready/In Progress/Completed]

â¸»

ðŸ“š ESSENTIAL CONTEXT

CRITICAL: Read these files before starting the implementation to gain valuable context:

[list files here]
    â¸»

ðŸ§  Quick Overview (â‰¤3 sentences)

Why this task exists, what it delivers, and the high-level outcome.
Keep under 200 words so Codex can understand purpose immediately.

â¸»


ðŸŽ¯ Goal of the Task

Clear, outcome-driven statement of what success looks like.

â¸»

ðŸš€ Run This (Local Dev & Test Commands)

Exact commands to run the task end-to-end.

uv venv
source .venv/bin/activate
uv pip install -r requirements.txt
# To start database locally, if needed:
docker compose up -d db
# Start API server
uvicorn app.main:app --reload
# Run tests
pytest

Include environment variables, mock setup, or seed data if required.

â¸»

ðŸ§° Local Development
	â€¢	Setup: Commands and services needed
	â€¢	Seed Data: What data gets created and why
	â€¢	Env Vars: .env.example updated
	â€¢	Common Issues: Troubleshooting notes

â¸»

Development Practices
	â€¢	Trunk-based: Work directly on main branch
	â€¢	Commit limit: â‰¤1000 lines per commit
	â€¢	Independent tasks: Each must run/test in isolation

â¸»

ðŸ§ª BDD Scenario

Feature: [Feature Name]
As a [user type]
I want to [goal]
So that [benefit]

Scenario: [Main scenario]
Given [precondition]
When [action]
Then [expected result]

â¸»

âœ… Acceptance Criteria

Each must be testable and map directly to a test or check.
	â€¢	Functional: [Expected behavior or outcome]
	â€¢	Error Handling: [All API errors (from Exa, You.com, or Tavily) are logged with context; user receives actionable message]
	â€¢	Performance: [Latency/throughput targets, e.g. P95 < 300ms]
	â€¢	Security: [API keys never printed to log output or returned in API responses]
	â€¢	Observability: [Success and error logs visible in Render dashboard; DB insertions confirmed in Supabase]
	â€¢	Documentation: [README or inline docs updated]

â¸»

âš™ï¸ Files & Resources
    â€¢ Files Affected: [List of source files to create/modify]
    â€¢ Dependencies:
        â€“ Tasks: [Upstream FSQs or blocker tasks that must land first]
        â€“ Environment: [Env vars, services, or fixtures required]
    â€¢ External Resources: [e.g., Exa API docs, Tavily API docs, Render dashboard, Supabase dashboard]
    â€¢ Contracts/IO Shapes: [Request/Response examples or schemas referenced]
	

â¸»

ðŸ§± Inputs & Outputs (Contracts)
	â€¢	Request: Example JSON or type definition
	â€¢	Response: Expected JSON or schema
	â€¢	Error Codes: Explicit list with messages
	â€¢	Idempotency/Versioning: Strategy to prevent duplicates or drift

â¸»

ðŸ’¼ Business Context
	â€¢	Value: Why this matters to the business
	â€¢	Risk: What could go wrong
	â€¢	Success Metrics: How to know itâ€™s done and high quality
    â€¢	Hypothesis: â€œMulti-source, timestamped, explainable leads convert faster and yield higher user trust.